# -*- coding: utf-8 -*-
"""Parcial_2_Exploracion_DP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1z7hH6-Oydhd-98K_jeq6Y026_SguJrGf

#***Exploración y preprocesamiento de datos – Parcial 2***
----
>**Author:**
* Daniela Pinzon

##***Data Salary-Survey-EU-2020***
----
>**1.**  Crear transformación de imputación para cada uno de los tipos de variables.
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import MinMaxScaler, StandardScaler, OrdinalEncoder, OneHotEncoder ,RobustScaler
from sklearn.compose import ColumnTransformer
import joblib

"""###***Pandas And Numpy***
----
"""

#Limpiamos el data con pandas
df = pd.read_csv('/content/IT Salary Survey EU 2020.csv')

df

df.info()

df.isnull().sum()

# Filtrado de datos por columna - cols_num todos los que sean diferentes a object
cols_num = df.columns[~(df.dtypes==object).to_numpy()].to_numpy()

cols_num

# Filtrado de datos por columna - cols_cat todos los que sean diferentes a float
cols_cat = df.columns[~(df.dtypes==float).to_numpy()].to_numpy()

cols_cat

# Definir las columnas numéricas y categóricas
columnas_numericas = ['Age', 'Yearly brutto salary (without bonus and stocks) in EUR',
       'Annual brutto salary (without bonus and stocks) one year ago. Only answer if staying in the same country',
       'Have you been forced to have a shorter working week (Kurzarbeit)? If yes, how many hours per week']
columnas_categoricas = ['Timestamp', 'Gender', 'City', 'Position ',
       'Total years of experience', 'Years of experience in Germany',
       'Seniority level', 'Your main technology / programming language',
       'Other technologies/programming languages you use often',
       'Yearly bonus + stocks in EUR',
       'Annual bonus+stocks one year ago. Only answer if staying in same country',
       'Number of vacation days', 'Employment status',
       'Сontract duration', 'Main language at work', 'Company size',
       'Company type',
       'Have you lost your job due to the coronavirus outbreak?',
       'Have you received additional monetary support from your employer due to Work From Home? If yes, how much in 2020 in EUR']

"""###***Sklearn Simple Impute***
----
"""

# Crear instancia de SimpleImputer para variables numéricas (usando la media como estrategia)
imputer_numericas = SimpleImputer(strategy='mean')

# Crear instancia de SimpleImputer para variables categóricas (usando la moda como estrategia)
imputer_categoricas = SimpleImputer(strategy='most_frequent')

# Aplicar la transformación de imputación a las columnas numéricas
df[columnas_numericas] = imputer_numericas.fit_transform(df[columnas_numericas])

# Aplicar la transformación de imputación a las columnas categóricas
df[columnas_categoricas] = imputer_categoricas.fit_transform(df[columnas_categoricas])

df

df.isnull().sum()

"""###***Sklearn Escalados Numericos***
----

>**2.**  Probar los respectivos escalados en los datos numéricos(maxMin,Standard, Robust)
"""

columnas_numericas = df[columnas_numericas]

columnas_numericas

# Crear un objeto de escalador Max-Min
scaler_maxmin = MinMaxScaler()

# Escalar los datos utilizando Max-Min
df_maxmin = scaler_maxmin.fit_transform(columnas_numericas)
df_maxmin = pd.DataFrame(df_maxmin, columns=columnas_numericas.columns)

# Imprimir los DataFrames escalados
print("Datos escalados con Max-Min:")
df_maxmin

# Crear un objeto de escalador Standard
scaler_standard = StandardScaler()

# Escalar los datos utilizando Standard
df_standard = scaler_standard.fit_transform(columnas_numericas)
df_standard = pd.DataFrame(df_standard, columns=columnas_numericas.columns)

print("\nDatos escalados con Standard:")
df_standard

# Crear un objeto de escalador Robust
scaler_robust = RobustScaler()

# Escalar los datos utilizando Robust
df_robust = scaler_robust.fit_transform(columnas_numericas)
df_robust = pd.DataFrame(df_robust, columns=columnas_numericas.columns)

print("\nDatos escalados con Robust:")
df_robust

"""###***Sklearn Escalados Categoricos***
----

>**3.**  Determinar la mejor transformación categórica(onehot vs ordinal).  Procesar los string de las variables categóricas(por ej. Lenguaje de programación) para un mejor resultado.
"""

columnas_categoricas = df[columnas_categoricas]

columnas_categoricas

columnas_categoricas.info()

# Combinacion de transformadores es una clase porque piensa en mayusculas
columnTransformer = ColumnTransformer(
    [
        # nombre , clase (constructor),arreglo
        # One-Hot Encoding : Es una técnica utilizada para convertir variables categóricas en una representación numérica binaria.
        # Cada categoría se convierte en una nueva columna binaria (0 o 1), y se le asigna un valor de 1 en la columna correspondiente
        # si la muestra pertenece a esa categoría, y un valor de 0 en las demás columnas.
       ('OneHot',OneHotEncoder(sparse = False),[0,2,4,5,7,8,9,10,11,18]),
        # Ordinal Encoding : Es una técnica utilizada para codificar variables categóricas en forma de enteros ordinales,
        # donde las categorías se asignan a enteros numéricos en función de su orden o jerarquía.
       ('Ordinal',OrdinalEncoder(),[1,3,6,12,13,14,15,16,17])
    ]
)

df['Сontract duration'].unique()

df['City'].unique()

df['Seniority level'].unique()

columnTransformer.fit(df)

col_trans = columnTransformer.transform(df)

col_trans

"""###***Distribucion Uniforme - Box-Cox***
----

>**4.**  Hacer la distribución de las variables numéricas más Gaussianas.
"""

# Distribucion inicial de las variables numericas - Correlacion con matriz
pd.plotting.scatter_matrix(columnas_numericas, figsize=(30,10))
plt.show()

# Aplicar transformación Box-Cox a las variables numéricas
from scipy.stats import boxcox

df_transformed = df.copy() # Crear una copia del DataFrame original para almacenar las variables transformadas

# Aplicar transformación Box-Cox a las variables numéricas
for col in columnas_numericas:
    df_transformed[col], _ = boxcox(df[col] + 1) # Agregar 1 a los valores para evitar valores negativos en la transformación

# Visualizar la nueva distribución de las variables numéricas transformadas
plt.figure(figsize=(20, 2))
sns.histplot(data=df_transformed, kde=True)
plt.title('Distribución de las Variables Numéricas Transformadas')
plt.show()

"""###***Power Transformer YEO***
----
"""

from scipy.stats import skewnorm
from sklearn.preprocessing import PowerTransformer

columnas_numericas_c =columnas_numericas.copy()

pt = PowerTransformer(standardize= False)

pt.fit(columnas_numericas)

pt_trans = pt.transform(columnas_numericas)

# Se genera distribucion de power transformer
pt_trans

# a Histograma de la nueva distribucion
sns.histplot(pt_trans)

"""###***Power Transformer Box-cox***
----
"""

# Metodo perform a box-cox
pt_box = PowerTransformer(method='box-cox', standardize= False)

pt_box.fit(columnas_numericas_c + 1) # Agregar 1 a los valores para evitar valores negativos en la transformación

pt_box_trans = pt_box.transform(columnas_numericas_c + 1)

pt_box_trans

# a Histograma de la nueva distribucion con box-cox
sns.histplot(pt_box_trans)

"""###***Pipeline***
----

>**5.**  Crear un pipeline Numérico y otro Categórico. Unir ambos en un solo pipeline usando ColumnTransformer.
"""

# Crear el pipeline numérico
pipeline_num = Pipeline([
    ('imputer', SimpleImputer(strategy='median')), # Manejo de valores faltantes
    ('scaler', MinMaxScaler()) # Escalado de características numéricas
])

# Crear el pipeline categórico
pipeline_cat = Pipeline([
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder())
])

col_num = [1,10,12,21]
col_cat = [0,2,3,4,5,6,7,8,9,11,13,14,15,16,17,18,19,20,22]

# Unimos los pipelines usando column Transformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', pipeline_num, col_num),
        ('cat', pipeline_cat, col_cat)
    ])

pipeline = Pipeline([
    ('preprocessor', preprocessor)
])

preprocessor

tm_df = preprocessor.fit_transform(df)

tm_df

"""###***Joblib***
----

>**6.** Seleccionar variables más importantes y eliminar las restantes en el pipeline(eliminar todas las columnas relacionadas con Salary, ya que es la variable a predecir)
"""

df.info()

# Eliminamos las columnas relacionadas a salary e inecesarias
# Separamos las variables predictoras (df_l) y la variable a predecir
df_l = df.drop(['Timestamp','City','Yearly bonus + stocks in EUR','Annual brutto salary (without bonus and stocks) one year ago. Only answer if staying in the same country',
         'Annual bonus+stocks one year ago. Only answer if staying in same country','Other technologies/programming languages you use often','Company type','Company size','Years of experience in Germany','Number of vacation days','Employment status' ,
         'Have you lost your job due to the coronavirus outbreak?','Main language at work','Have you been forced to have a shorter working week (Kurzarbeit)? If yes, how many hours per week','Have you received additional monetary support from your employer due to Work From Home? If yes, how much in 2020 in EUR'], axis=1)

df_l.info()

"""##***Prediccion xgb***
----
"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

variable_predictoria = df_l['Yearly brutto salary (without bonus and stocks) in EUR']

cols_num = df_l.columns[~(df_l.dtypes==object).to_numpy()].to_numpy()

cols_num

# Dividir los datos en conjuntos de entrenamiento y prueba
X = df_l[['Yearly brutto salary (without bonus and stocks) in EUR']]  # Variables predictoras
y = variable_predictoria  # Variable objetivo
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

import xgboost as xgb
modelo = xgb.XGBRegressor()  # Crear instancia de un modelo XGBoost
modelo.fit(X_train, y_train)  # Entrenar el modelo con los datos de entrenamiento

# Evaluar el modelo
y_pred = modelo.predict(X_test)  # Realizar predicciones en el conjunto de prueba
mse = mean_squared_error(y_test, y_pred)  # Calcular el error cuadrático medio
print("Error cuadrático medio:", mse)  # Mostrar el error cuadrático medio

# Predecir nuevos datos
# Evaluar el modelo
y_pred = modelo.predict(X_test)  # Realizar predicciones en el conjunto de prueba
mse = mean_squared_error(y_test, y_pred)  # Calcular el error cuadrático medio
print("Error cuadrático medio:", mse)  # Mostrar el error cuadrático medio

"""##***Prediccion lineal -sklearn***
----
"""

modelo = LinearRegression()  # Crear instancia de un modelo de regresión lineal
modelo.fit(X_train, y_train)  # Entrenar el modelo con los datos de entrenamiento

#Evaluar el modelo
y_pred = modelo.predict(X_test)  # Realizar predicciones en el conjunto de prueba
mse = mean_squared_error(y_test, y_pred)  # Calcular el error cuadrático medio
print("Error cuadrático medio:", mse)  # Mostrar el error cuadrático medio

# Predecir nuevos datos
nuevos_datos = modelo.predict(X_test)
prediccion = modelo.predict(nuevos_datos.reshape(-1, 1))
print("Predicción:", prediccion)  # Mostrar la predicción para nuevos datos

"""###***Rolling Mean y Joblib Dumb***
----

>**7.** Guardar el pipeline como joblib.
"""

joblib.dump(pipeline, "pipeline_1.joblib")

"""###***Pipeline - Joblib-load***
----

>**8.** Cargar el pipeline y probarlo con un dato de entrada.
"""

pipeline = joblib.load('pipeline_1.joblib')

pipeline

#Escalamiento de datos con nuevo dato de entrada
# Datos de entrada para probar el pipeline
#  Se usa para recuperar datos financieros de Yahoo Finance. Proporciona una manera simple y conveniente de obtener datos históricos de acciones,
#  estados financieros y otra información financiera de la API de Yahoo Finance.
import yfinance as yf

# Fechas en las que queremos visualizar el comportamiento de las acciones
start = '2020-01-01'
end = '2023-03-31'
# Especifica el símbolo o ticker de la acción que deseas obtener // en este caso apple
symbol = 'AAPL'
# Guardamos en data el historico de las acciones
stock_data = yf.Ticker(symbol)
stock_data = stock_data.history(start=start, end=end)

stock_data

ts = stock_data.High

ts

ts["2022"]

ts["2022"].plot()

# Crear una serie de tiempo ficticia
serie_tiempo = pd.Series(ts["2022"])

# Calculamos la media móvil con una ventana de tamaño 3
media_movil = stock_data["Media Movil"]= serie_tiempo.rolling(window=3).mean()

stock_data

# Comparacion entre la serie original y la media movil originada
# Mostrar la serie original y la media móvil resultante
print("Serie de tiempo original:")
print(serie_tiempo)
print("\nMedia móvil con ventana de tamaño", 3)
print(media_movil)

plt.title('Comparacion entre media y original')
serie_tiempo.plot()
media_movil.plot()

serie_tiempo.plot()
a = (ts["2022"].rolling(window=20).mean())+2*(ts["2022"].rolling(window=20).std())
b = (ts["2022"].rolling(window=20).mean())-2*(ts["2022"].rolling(window=20).std())
a.plot()
b.plot()

last_months = ts["2022"].resample('M').last()
last_months

retorno = stock_data["t-1"] = ts.shift(1)

stock_data

stock_data["t-1"] = last_months.shift(1)
stock_data["Retorno mensual"] = (((serie_tiempo - retorno)/retorno*100))

sns.barplot(data=stock_data, x=stock_data.index.month , y=stock_data["Retorno mensual"])

stock_data.info()

# Definimos los datos de prueba del stock data como datos de entrada
col_num = [0,1,2,3,4,5,6,7,8,9]
col_cat = []

# Unimos los pipelines usando column Transformer
pipeline = ColumnTransformer(
    transformers=[
        ('num', pipeline_num, col_num),
        ('cat', pipeline_cat, col_cat)
    ])

pipeline

# Usamos esos datos de entrada para usar el pipeline
tm_df = pipeline.fit_transform(stock_data)

tm_df

tm_df = pd.DataFrame(tm_df)

sns.pairplot(tm_df)